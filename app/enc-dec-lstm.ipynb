{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nimport random\nimport math\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.711795Z","iopub.execute_input":"2025-05-08T08:27:09.712145Z","iopub.status.idle":"2025-05-08T08:27:09.717989Z","shell.execute_reply.started":"2025-05-08T08:27:09.712119Z","shell.execute_reply":"2025-05-08T08:27:09.717129Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n        super().__init__()\n\n        self.hid_dim = hid_dim\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        # src = [src len, batch size]\n\n        embedded = self.dropout(self.embedding(src))\n        # embedded = [src len, batch size, emb dim]\n\n        outputs, (hidden, cell) = self.rnn(embedded)\n        # outputs = [src len, batch size, hid dim * n directions] # if bidirectional\n        # hidden = [n layers * n directions, batch size, hid dim]\n        # cell = [n layers * n directions, batch size, hid dim]\n\n        # outputs are always from the top hidden layer\n        return outputs, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.719309Z","iopub.execute_input":"2025-05-08T08:27:09.719552Z","iopub.status.idle":"2025-05-08T08:27:09.734814Z","shell.execute_reply.started":"2025-05-08T08:27:09.719530Z","shell.execute_reply":"2025-05-08T08:27:09.734082Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n\n        self.attn = nn.Linear((enc_hid_dim) + dec_hid_dim, dec_hid_dim) # If encoder is bidirectional, use enc_hid_dim * 2\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden = [1, batch size, dec hid dim] # If decoder is single layer\n        # encoder_outputs = [src len, batch size, enc hid dim] # If encoder is single layer, use enc_hid_dim * 2 if bidirectional\n\n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n\n        # Repeat decoder hidden state src_len times\n        hidden = hidden.squeeze(0).unsqueeze(0).repeat(src_len, 1, 1)\n        # hidden = [src len, batch size, dec hid dim]\n\n        # Calculate energy between hidden and encoder outputs\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        # energy = [src len, batch size, dec hid dim]\n\n        # Calculate attention weights\n        attention = self.v(energy).squeeze(2)\n        # attention = [src len, batch size]\n\n        return torch.softmax(attention, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.735546Z","iopub.execute_input":"2025-05-08T08:27:09.735739Z","iopub.status.idle":"2025-05-08T08:27:09.751970Z","shell.execute_reply.started":"2025-05-08T08:27:09.735718Z","shell.execute_reply":"2025-05-08T08:27:09.751258Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, dropout=dropout) # If encoder is bidirectional, use hid_dim * 2\n        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim) # If encoder is bidirectional, use hid_dim * 3\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        # input = [batch size]\n        # hidden = [1, batch size, dec hid dim]\n        # cell = [1, batch size, dec hid dim]\n        # encoder_outputs = [src len, batch size, enc hid dim]\n\n        input = input.unsqueeze(0)\n        # input = [1, batch size]\n\n        embedded = self.dropout(self.embedding(input))\n        # embedded = [1, batch size, emb dim]\n\n        # Calculate attention weights\n        # attention_weights = [src len, batch size]\n        attention_weights = self.attention(hidden, encoder_outputs)\n\n        # --- FIX START ---\n        # Permute attention weights to have batch_size first: [batch size, src len]\n        attention_weights = attention_weights.permute(1, 0)\n        # attention_weights = [batch size, src len]\n\n        # Unsqueeze to add a dimension for matrix multiplication: [batch size, 1, src len]\n        attention_weights = attention_weights.unsqueeze(1)\n        # attention_weights = [batch size, 1, src len]\n\n        # Permute encoder_outputs to have batch_size first: [batch size, src len, enc hid dim]\n        # This line was already correct for bmm\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        # encoder_outputs = [batch size, src len, enc hid dim]\n\n        # Perform batch matrix multiplication: (batch size, 1, src len) x (batch size, src len, enc hid dim)\n        weighted_context = torch.bmm(attention_weights, encoder_outputs).squeeze(1)\n        # weighted_context = [batch size, enc hid dim]\n        # --- FIX END ---\n\n        weighted_context = weighted_context.unsqueeze(0)\n        # weighted_context = [1, batch size, enc hid dim]\n\n        rnn_input = torch.cat((embedded, weighted_context), dim=2)\n        # rnn_input = [1, batch size, emb dim + enc hid dim]\n\n        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n        # output = [1, batch size, hid dim]\n        # hidden = [1, batch size, hid dim]\n        # cell = [1, batch size, hid dim]\n\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_context = weighted_context.squeeze(0) # Squeeze again after unsqueeze(0) for concat\n\n        prediction = self.fc_out(torch.cat((output, weighted_context, embedded), dim=1))\n        # prediction = [batch size, output dim]\n\n        # Return attention weights in a more usable shape if needed, e.g., [batch size, src len]\n        # Let's return the shape after the first permute: [batch size, src len]\n        return prediction, hidden, cell, attention_weights.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.753156Z","iopub.execute_input":"2025-05-08T08:27:09.753422Z","iopub.status.idle":"2025-05-08T08:27:09.770216Z","shell.execute_reply.started":"2025-05-08T08:27:09.753402Z","shell.execute_reply":"2025-05-08T08:27:09.769524Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n        assert encoder.hid_dim == decoder.rnn.hidden_size, \\\n            \"Hidden dimensions of encoder and decoder must be equal!\"\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src = [src len, batch size]\n        # trg = [trg len, batch size]\n        # teacher_forcing_ratio is probability to use ground-truth target word\n\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        # Tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n\n        # Encoder outputs, and final hidden and cell states\n        encoder_outputs, hidden, cell = self.encoder(src)\n\n        # First input to the decoder is the <sos> token\n        input = trg[0, :]\n\n        for t in range(1, trg_len):\n            # Get output prediction from decoder\n            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n\n            # Store prediction\n            outputs[t] = output\n\n            # Decide if we will use teacher forcing\n            teacher_force = random.random() < teacher_forcing_ratio\n\n            # Get the highest predicted token from our predictions\n            top1 = output.argmax(1)\n\n            # If teacher forcing, use actual next token, otherwise use predicted token\n            input = trg[t, :] if teacher_force else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.770976Z","iopub.execute_input":"2025-05-08T08:27:09.771308Z","iopub.status.idle":"2025-05-08T08:27:09.786500Z","shell.execute_reply.started":"2025-05-08T08:27:09.771281Z","shell.execute_reply":"2025-05-08T08:27:09.785762Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_cnn_dailymail_dataset(num_train_samples=None, num_val_samples=None):\n    # Using Hugging Face datasets for easy access\n    print(\"Loading CNN/DailyMail dataset...\")\n    dataset = load_dataset(\"cnn_dailymail\", '3.0.0') # Specify version\n\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # Or a summarization-specific tokenizer\n\n    # Add special tokens for start and end of sequence\n    tokenizer.add_special_tokens({'bos_token': '<sos>', 'eos_token': '<eos>'})\n\n    def tokenize_function(examples):\n        # Tokenize articles\n        model_inputs = tokenizer(examples['article'], max_length=512, truncation=True, padding=\"max_length\")\n        # Tokenize highlights (summaries)\n        labels = tokenizer(examples['highlights'], max_length=128, truncation=True, padding=\"max_length\")\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    print(\"Tokenizing dataset...\")\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n    # Select a subset of data if specified\n    if num_train_samples is not None:\n        tokenized_datasets['train'] = tokenized_datasets['train'].select(range(num_train_samples))\n        print(f\"Using {len(tokenized_datasets['train'])} training samples.\")\n    if num_val_samples is not None:\n        tokenized_datasets['validation'] = tokenized_datasets['validation'].select(range(num_val_samples))\n        print(f\"Using {len(tokenized_datasets['validation'])} validation samples.\")\n\n    # Remove original text columns\n    tokenized_datasets = tokenized_datasets.remove_columns([\"article\", \"highlights\", \"id\"])\n    tokenized_datasets.set_format(\"torch\")\n\n    return tokenized_datasets, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.896504Z","iopub.execute_input":"2025-05-08T08:27:09.896812Z","iopub.status.idle":"2025-05-08T08:27:09.903513Z","shell.execute_reply.started":"2025-05-08T08:27:09.896786Z","shell.execute_reply":"2025-05-08T08:27:09.902733Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training Function\ndef train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n\n    for batch in tqdm(iterator, desc=\"Training\"):\n        src = batch['input_ids'].transpose(0, 1).to(model.device) # [src len, batch size]\n        trg = batch['labels'].transpose(0, 1).to(model.device)   # [trg len, batch size]\n\n        optimizer.zero_grad()\n\n        output = model(src, trg)\n        # output = [trg len, batch size, output dim]\n\n        output_dim = output.shape[-1]\n\n        # Reshape for criterion: (N, C) where N is number of predictions, C is number of classes\n        # Use .reshape() instead of .view()\n        output = output[1:].reshape(-1, output_dim)\n        trg = trg[1:].reshape(-1)\n\n        # trg = [(trg len - 1) * batch size]\n        # output = [(trg len - 1) * batch size, output dim]\n\n        loss = criterion(output, trg)\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.904691Z","iopub.execute_input":"2025-05-08T08:27:09.904935Z","iopub.status.idle":"2025-05-08T08:27:09.923147Z","shell.execute_reply.started":"2025-05-08T08:27:09.904919Z","shell.execute_reply":"2025-05-08T08:27:09.922482Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Evaluation Function\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for batch in tqdm(iterator, desc=\"Evaluating\"):\n            src = batch['input_ids'].transpose(0, 1).to(model.device) # [src len, batch size]\n            trg = batch['labels'].transpose(0, 1).to(model.device)   # [trg len, batch size]\n\n            output = model(src, trg, 0) # Turn off teacher forcing for evaluation\n            # output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n\n            # Use .reshape() instead of .view()\n            output = output[1:].reshape(-1, output_dim)\n            trg = trg[1:].reshape(-1)\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.923878Z","iopub.execute_input":"2025-05-08T08:27:09.924117Z","iopub.status.idle":"2025-05-08T08:27:09.942746Z","shell.execute_reply.started":"2025-05-08T08:27:09.924099Z","shell.execute_reply":"2025-05-08T08:27:09.941988Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # --- Configuration ---\n    NUM_TRAIN = 5000 # Set your desired number of training samples\n    NUM_VAL = 1000    # Set your desired number of validation samples\n    BATCH_SIZE = 32\n    ENC_EMB_DIM = 256\n    DEC_EMB_DIM = 256\n    HID_DIM = 512\n    ENC_DROPOUT = 0.5\n    DEC_DROPOUT = 0.5\n    N_EPOCHS = 20\n    CLIP = 1.0\n\n    # Load and preprocess data\n    tokenized_datasets, tokenizer = load_cnn_dailymail_dataset(num_train_samples=NUM_TRAIN, num_val_samples=NUM_VAL)\n\n    # Create DataLoaders\n    train_iterator = DataLoader(tokenized_datasets['train'], batch_size=BATCH_SIZE, shuffle=True)\n    valid_iterator = DataLoader(tokenized_datasets['validation'], batch_size=BATCH_SIZE)\n\n    INPUT_DIM = len(tokenizer)\n    OUTPUT_DIM = len(tokenizer)\n    TRG_PAD_IDX = tokenizer.pad_token_id\n\n    # Initialize model components\n    attention = Attention(HID_DIM, HID_DIM) # Use HID_DIM for both if encoder is not bidirectional\n    encoder = EncoderRNN(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n    decoder = DecoderRNN(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT, attention)\n\n    model = Seq2Seq(encoder, decoder, device).to(device)\n\n    # Initialize weights\n    def init_weights(m):\n        for name, param in m.named_parameters():\n            if 'weight' in name:\n                nn.init.normal_(param.data, mean=0, std=0.01)\n            else:\n                nn.init.constant_(param.data, 0)\n\n    model.apply(init_weights)\n\n    # Optimizer and Loss Function\n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX) # Ignore padding index in loss calculation\n\n    best_valid_loss = float('inf')\n\n    # Training Loop\n    for epoch in range(N_EPOCHS):\n        start_time = time.time()\n\n        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n        valid_loss = evaluate(model, valid_iterator, criterion)\n\n        end_time = time.time()\n\n        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n\n        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}')\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best_seq2seq_attn_model.pt')\n\n    print(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.944030Z","iopub.execute_input":"2025-05-08T08:27:09.944347Z","iopub.status.idle":"2025-05-08T08:27:09.958689Z","shell.execute_reply.started":"2025-05-08T08:27:09.944328Z","shell.execute_reply":"2025-05-08T08:27:09.957955Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:09.959395Z","iopub.execute_input":"2025-05-08T08:27:09.959625Z","iopub.status.idle":"2025-05-08T10:45:50.140696Z","shell.execute_reply.started":"2025-05-08T08:27:09.959604Z","shell.execute_reply":"2025-05-08T10:45:50.140042Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading CNN/DailyMail dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9423e0a7c6954e9c8e33e3eeb98625b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e288338a515a4565812928dbee3e2768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f543fd021f54fd59be0bf2ce63dc6b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4719a69054a4e94a64ddd3a2f047330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf233ce54a64f579dbee49707d03d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d839af4c1854910898645dbb23d733a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc050a20367f430787daf390144ee46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69b6f0c53bf4fd7a8fa9bd566db1771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d22a6c064a482183ac20230eec574c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84ef36368e74b72b109b7d7b858befb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08797bf609b4bcf8bbfaee3050e84b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e31b364de6994119ba7057d433212207"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d9143d3497488287dfd312c8030135"}},"metadata":{}},{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27f255d9500432a9a074da96727971f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e1c48f693884f0eb79c35fb4644e57b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5652e81ea31d454bb4ee648813569026"}},"metadata":{}},{"name":"stdout","text":"Using 5000 training samples.\nUsing 1000 validation samples.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\nTraining: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 01 | Time: 6m 33s\n\tTrain Loss: 7.498 | Train PPL: 1803.951\n\t Val. Loss: 7.396 |  Val. PPL: 1629.738\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 02 | Time: 6m 32s\n\tTrain Loss: 7.028 | Train PPL: 1127.268\n\t Val. Loss: 7.399 |  Val. PPL: 1634.704\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03 | Time: 6m 32s\n\tTrain Loss: 6.834 | Train PPL: 928.608\n\t Val. Loss: 7.393 |  Val. PPL: 1623.780\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 04 | Time: 6m 32s\n\tTrain Loss: 6.646 | Train PPL: 769.546\n\t Val. Loss: 7.416 |  Val. PPL: 1663.107\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 6m 32s\n\tTrain Loss: 6.456 | Train PPL: 636.313\n\t Val. Loss: 7.437 |  Val. PPL: 1697.385\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 06 | Time: 6m 32s\n\tTrain Loss: 6.288 | Train PPL: 538.101\n\t Val. Loss: 7.489 |  Val. PPL: 1787.481\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 07 | Time: 6m 33s\n\tTrain Loss: 6.096 | Train PPL: 443.925\n\t Val. Loss: 7.523 |  Val. PPL: 1850.563\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 08 | Time: 6m 32s\n\tTrain Loss: 5.913 | Train PPL: 369.863\n\t Val. Loss: 7.547 |  Val. PPL: 1895.365\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 09 | Time: 6m 33s\n\tTrain Loss: 5.751 | Train PPL: 314.580\n\t Val. Loss: 7.599 |  Val. PPL: 1995.844\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10 | Time: 6m 33s\n\tTrain Loss: 5.548 | Train PPL: 256.834\n\t Val. Loss: 7.683 |  Val. PPL: 2170.626\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11 | Time: 6m 33s\n\tTrain Loss: 5.347 | Train PPL: 210.024\n\t Val. Loss: 7.763 |  Val. PPL: 2352.056\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12 | Time: 6m 33s\n\tTrain Loss: 5.131 | Train PPL: 169.124\n\t Val. Loss: 7.848 |  Val. PPL: 2561.542\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13 | Time: 6m 33s\n\tTrain Loss: 4.937 | Train PPL: 139.312\n\t Val. Loss: 7.922 |  Val. PPL: 2757.144\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 14 | Time: 6m 33s\n\tTrain Loss: 4.730 | Train PPL: 113.324\n\t Val. Loss: 8.063 |  Val. PPL: 3175.878\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 15 | Time: 6m 33s\n\tTrain Loss: 4.527 | Train PPL: 92.466\n\t Val. Loss: 8.133 |  Val. PPL: 3403.914\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 16 | Time: 6m 32s\n\tTrain Loss: 4.327 | Train PPL: 75.683\n\t Val. Loss: 8.225 |  Val. PPL: 3734.615\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 17 | Time: 6m 32s\n\tTrain Loss: 4.125 | Train PPL: 61.857\n\t Val. Loss: 8.367 |  Val. PPL: 4304.701\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 18 | Time: 6m 32s\n\tTrain Loss: 3.948 | Train PPL: 51.857\n\t Val. Loss: 8.458 |  Val. PPL: 4713.697\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:01<00:00,  2.30s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 19 | Time: 6m 32s\n\tTrain Loss: 3.765 | Train PPL: 43.184\n\t Val. Loss: 8.548 |  Val. PPL: 5158.556\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 157/157 [06:02<00:00,  2.31s/it]\nEvaluating: 100%|██████████| 32/32 [00:30<00:00,  1.05it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 20 | Time: 6m 33s\n\tTrain Loss: 3.591 | Train PPL: 36.284\n\t Val. Loss: 8.647 |  Val. PPL: 5693.361\nTraining finished.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}